{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tWhat is Boosting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Boosting is type of ensemble technique which reduces high bais or training error of base learner with\n",
    "the help of additive addition of residual loss.Here the residual error of last stage model is used to train\n",
    "the next stage model i.e weak learners are added to give final prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\tHow do boosting and bagging differ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Boosting can minimize all kind of loss which are differentiable and inturn reduces the bais.\n",
    "However bagging tries to reduce variance and can minimize only one type of loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\tWhat are week and strong classifiers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A 'weak learner' is any ML algorithm (for regression/classification) that provides an accuracy slightly better than random guessing\n",
    "A strong learner is a classifier that is arbitrarily well-correlated with the true classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.\tWhy are trees deemed fit for boosting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Trees are deemed fit for boosting because\n",
    "1)Trees are robust to outliers\n",
    "2)Highly computational scaleable\n",
    "3)Handles missing values \n",
    "4)No scaling required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.\tExplain the step by step implementation of ADA Boost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Initialize the observation weights  $w_i=\\frac{1}{N}$ where $i = 1,2, \\dots, N$ for all the samples.\n",
    "2. For $m=1$ to $M$:\n",
    "    - fit a classifier $G_m(x)$ to the training data using weights $w_i$,\n",
    "    - compute $err_m = \\frac{\\sum_{i=1}^{N} w_i I (y_i \\neq G_m(x))}{\\sum_{i=1}^{N}w_i}$,\n",
    "    - compute $\\alpha_m = \\frac {1}{2} \\log (\\frac{(1-err_m)}{err_m})$. This is the contribution of that tree to the final result.\n",
    "    - calculate the new weights using the formula:\n",
    "    \n",
    "    $w_i \\leftarrow w_i \\cdot \\exp [\\alpha_m \\cdot I (y_i \\neq G_m(x)]$, where $i = 1,2, \\dots, N$\n",
    "- Normalize the new sample  weights so that their sum is 1.\n",
    "- Construct the next tree using the new weights\n",
    "\n",
    "\n",
    " 3. At the end, compare the summation of results from all the trees and the final result is either the one with the highest sum(for regression) or it is the class which has the most weighted voted average(for classification).\n",
    "\n",
    "       Output $G_m(x) = argmax [\\sum_{m=1}^{M} \\alpha_m G_m(x)]$ (Regression)\n",
    "\n",
    "       Output $G_m(x) = sigm [\\sum_{m=1}^{M} \\alpha_m G_m(x)]$ (Classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.\tWhat are pseudo residuals?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pseudo residuals are negative gardient or derivative of loss function of model at any stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.\tExplain the step by step implementation of Gradient boosted trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1 Consider Mo to be base learner DT with shallow depth i.e max_depth=2 or 1.which predicts \n",
    "output on the basis of average error.\n",
    "Step 2 Calculate the residual error for next stage from \n",
    "Pseudo residual= actual label- the predicted result (which is average in the first iteration)\n",
    "Step 3 create a tree to predict the pseudo residuals instead of a tree to predict for the actual column values.\n",
    "         new result= previous result+learning rate* residual\n",
    "Mathematically,  ğ¹1(ğ‘¥)=ğ¹0(ğ‘¥)+ğœˆâˆ‘ğ›¾ \n",
    "where  ğœˆ  is the learning rate and  ğ›¾  is the residual\n",
    "\n",
    "Repeat these steps until the residual stops decreasing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.\tExplain the step by step implementation of XGBoost Algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise the tree with only one leaf.\n",
    "compute the similarity using the formula\n",
    "ğ‘†ğ‘–ğ‘šğ‘–ğ‘™ğ‘ğ‘Ÿğ‘–ğ‘¡ğ‘¦=ğºğ‘Ÿğ‘ğ‘‘ğ‘–ğ‘’ğ‘›ğ‘¡2â„ğ‘’ğ‘ ğ‘ ğ‘–ğ‘ğ‘›+ğœ†\n",
    " \n",
    "Where  ğœ†  is the regularisation term.\n",
    "Now for splitting data into a tree form, calculate\n",
    "ğºğ‘ğ‘–ğ‘›=ğ‘™ğ‘’ğ‘“ğ‘¡ğ‘ ğ‘–ğ‘šğ‘–ğ‘™ğ‘ğ‘Ÿğ‘–ğ‘¡ğ‘¦+ğ‘Ÿğ‘–ğ‘”â„ğ‘¡ğ‘ ğ‘–ğ‘šğ‘–ğ‘™ğ‘ğ‘Ÿğ‘–ğ‘¡ğ‘¦âˆ’ğ‘ ğ‘–ğ‘šğ‘–ğ‘™ğ‘ğ‘Ÿğ‘–ğ‘¡ğ‘¦ğ‘“ğ‘œğ‘Ÿğ‘Ÿğ‘œğ‘œğ‘¡\n",
    " \n",
    "For tree pruning, the parameter  ğ›¾  is used. The algorithm starts from the lowest level of the tree and then starts pruning based on the value of  ğ›¾ .\n",
    "If  ğºğ‘ğ‘–ğ‘›âˆ’ğ›¾<0 , remove that branch. Else, keep the branch\n",
    "\n",
    "Learning is done using the equation\n",
    "ğ‘ğ‘’ğ‘¤ğ‘‰ğ‘ğ‘™ğ‘¢ğ‘’=ğ‘œğ‘™ğ‘‘ğ‘‰ğ‘ğ‘™ğ‘¢ğ‘’+ğœ‚âˆ—ğ‘ğ‘Ÿğ‘’ğ‘‘ğ‘–ğ‘ğ‘¡ğ‘–ğ‘œğ‘›\n",
    " \n",
    "where  ğœ‚  is the learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.\tWhat are the advantages of XGBoost?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "It implements all the benefits along with benefits of GBDT but one feature which makes it more powerful\n",
    "model as it uses column sampling and row sampling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
